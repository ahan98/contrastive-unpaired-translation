{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HULcL5diTr52"
   },
   "source": [
    "# CUT Architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9H9AjiOHTWQJ"
   },
   "source": [
    "Explained in **section 4** of [Park et al.](https://arxiv.org/abs/2007.15651), the architecture of CUT is largely the same as that of CycleGAN (see [Zhu et al.](https://arxiv.org/abs/1703.10593)).\n",
    "\n",
    "From CycleGAN, CUT reuses:\n",
    "- the Resnet-based generator, a variation of that from [Johnson et al.](https://arxiv.org/abs/1603.08155)\n",
    "- the 70x70 PatchGAN discriminator\n",
    "\n",
    "CUT differs from CycleGAN by:\n",
    "- using least-squares instead of binary cross entropy for the adversarial GAN loss\n",
    "- using PatchNCE loss instead of $\\ell_1$ cycle-consistency loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fNhtxcGEJgiK"
   },
   "source": [
    "\n",
    "## Generator\n",
    "\n",
    "The generator is based off the Resnet-based architecture from [Johnson et al.] with 9 residual blocks (assuming size 256x256 training images). The generator is partitioned into two sub-networks, encoder $G_{enc}$ and decoder $G_{dec}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sWP9fEYTXXgO"
   },
   "source": [
    "### Encoder\n",
    "\n",
    "$G_{enc}$ consists of the following layers:\n",
    "1. Reflection Padding\n",
    "    - one (3, 3, 3, 3) [reflection padding](https://pytorch.org/docs/master/generated/torch.nn.ReflectionPad2d.html) layer applied to the input image\n",
    "2. Convolutional Block (`c7s1-64`)\n",
    "    - one (7×7 Convolution)-(InstanceNorm)-(ReLU) block, with 64 ﬁlters, stride 1, and **no padding**\n",
    "3. Downsampling Block (`d128`)\n",
    "    - one (3×3 Convolution)-(InstanceNorm)-(ReLU) block, with 128 ﬁlters, stride 2, and zero-padding 1\n",
    "4. Downsampling Block (`d256`)\n",
    "    - one (3×3 Convolution)-(InstanceNorm)-(ReLU) block, with 256 ﬁlters, stride 2, and zero-padding 1\n",
    "5. Residual Block (`R256`)\n",
    "    - nine (3x3 Convolution)-(BatchNorm)-(ReLU)-(3x3 Convolution)-(BatchNorm) blocks, with 256 filters, stride 1, and **no padding**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FiLh9DYXZoo"
   },
   "source": [
    "### Decoder\n",
    "$G_{dec}$ consists of the following layers:\n",
    "1. Residual Block (`R256`)\n",
    "    - nine (3x3 Convolution)-(BatchNorm)-(ReLU)-(3x3 Convolution)-(BatchNorm) blocks, with 256 filters, stride 1, and **no padding**\n",
    "2. Upsampling Block (`u128`)\n",
    "    - one (3×3 Convolution)-(InstanceNorm)-(ReLU) block, with 128 ﬁlters, stride **1/2**, and zero-padding 1\n",
    "3. Upsampling Block (`u64`)\n",
    "    - one (3×3 Convolution)-(InstanceNorm)-(ReLU) block, with 64 ﬁlters, stride **1/2**, and zero-padding 1\n",
    "4. Convolutional Block (`c7s1-3`)\n",
    "    - one (7×7 Convolution)-(InstanceNorm)-(ReLU) block, with 3 ﬁlters, stride 1, and **no padding**\n",
    "5. Reflection Padding\n",
    "    - one (3, 3, 3, 3) reflection padding layer\n",
    "6. (Scaled) Tanh Activation\n",
    "    - one Tanh activation layer to scale the values back to a range of [0, 255], embedding the output into an RGB image\n",
    "\n",
    "> **NOTE**: The decoder acts as a symmetric reverse of the encoder. Even the fractionally-strided convolutional blocks in the decoder correspond to the integer-strided convolutional blocks in the encoder (for example, a 128x128 image with stride 1/2 is \"equivalent\" to a 256x256 image with stride 1).\n",
    "\n",
    "The naming conventions for the blocks (e.g., `c7s1-3`) is borrowed from Johnson et al.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jVCWbI6AKMnB"
   },
   "source": [
    "<img src=\"https://i.imgur.com/kBlcix2.png\" width=\"300\">\n",
    "\n",
    "Above is a visual depiction of the residual blocks used for the encoder and decoder. Notice the skip connection from the input to the output of the final BatchNorm. (Source: [Supplementary material](https://web.eecs.umich.edu/~justincj/papers/eccv16/JohnsonECCV16Supplementary.pdf) by Johnson et al.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zZrdjkIlJlTx"
   },
   "source": [
    "\n",
    "## Discriminator\n",
    "\n",
    "**Section 7.2** of Zhu et al. specifies the PatchGAN architecture:\n",
    "\n",
    "> For discriminator networks, we use 70 × 70 PatchGAN [22]. Let `Ck` denote a 4 × 4 Convolution-InstanceNorm-LeakyReLU layer with k ﬁlters and stride 2. After the last layer, we apply a convolution to produce a 1-dimensional output. We do not use InstanceNorm for the ﬁrst C64 layer. We use leaky ReLUs with a slope of 0.2. The discriminator architecture is: `C64-C128-C256-C512`\n",
    "\n",
    "Roughly speaking, the PatchGAN discriminator, created by [Isola et al.](https://arxiv.org/abs/1611.07004), is designed to better capture \"high frequency\" visual information. $\\ell_2$ and $\\ell_1$ losses capture low-frequency information well, but tend to result in blurry images. See this [Quora post](https://qr.ae/pN2yFq), explaining frequency for image processing.\n",
    "\n",
    "PatchGAN instead attempts to classify whether each patch (typically of size 70x70) is real or fake. Therefore, PatchGAN can be thought of as measuring style/texture loss. Isola et al. found that using PatchGAN generates higher quality images, runs faster, requires fewer hyperparameters, and works on images of arbitrary size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJyQ9pgzJopZ"
   },
   "source": [
    "## Multilayer Patchwise Contrastive Loss (PatchNCE)\n",
    "\n",
    "The newly introduced concept of patchwise contrastive loss is computed for a network consisting of two sub-networks: (i) the encoder $G_{enc}$, a subset of layers of $G$, and (ii) $H$, a two-layer MLP (multi-layer perceptron, i.e., fully-connected network). PatchNCE loss replaces the $\\ell_1$ cycle-consistency loss from CycleGAN. \n",
    "\n",
    "The loss is named as such (NCE, short for \"Noise Contrastive Estimation\") because:\n",
    "\n",
    "> \"These methods make use of noise contrastive estimation, learning an embedding where associated signals are brought together, in contrast to other samples in the dataset\" (Park et al. 4).\n",
    "\n",
    "In other words, we are maximizing the similarities between corresponding patches from the input and the embedded output, and minimizing *contrastive* patches (random non-corresponding patches from the input). As a rough example, the head of a generated zebra should be more similar to a zebra head from the ground truth dataset than irrelevant patches such as grass or sky.\n",
    "\n",
    "To compute PatchNCE loss, we use the softmax cross entropy loss to compute the probability of selecting the positive patches over the negative patches. The positive and negative patches are sampled from the latent space $H(G_{enc}(x))$, and the output patches are sampled from the latent space $H(G_{enc}(G(x)))$. \n",
    "\n",
    "**Appendix C, Section 1** explains that 256 random samples are drawn from each of the following layers:\n",
    "1. RGB pixels from the initial image input (size 1x1)\n",
    "2. `d128`, a downsampling convolution (size 9x9)\n",
    "3. `d256`, a downsampling convolution (size 15x15)\n",
    "4. the first of the nine residual blocks (size 35x35)\n",
    "5. the fifth `R256` residual block (size 99x99)\n",
    "\n",
    "For example, for the first feature extraction, since the input image has 3 channels (from RGB), and our receptive field size per sample is 1x1, we have a tensor of (256, 3, 1, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XT3CaSmIJq7r"
   },
   "source": [
    "## Adversarial GAN Loss\n",
    "\n",
    "$L_{GAN}$, the discriminator-generator adversarial loss, is computed using  [least-squares loss](https://arxiv.org/abs/1611.04076), instead of the standard binary cross-entropy loss defined by [Goodfellow et al.](https://papers.nips.cc/paper/5423-generative-adversarial-nets)\n",
    "\n",
    "The formula for Least-Squares GAN is:\n",
    "- $\\text{min}_D V_{LSGAN}(D) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[D(G(x))^2] + \\mathbb{E}_{y \\sim p_{\\text{data}}(y)}[(D(y) - 1)^2]$\n",
    "\n",
    "- $\\text{min}_G V_{LSGAN}(G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[(D(G(x)) - 1)^2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMrh8ZwSBUad"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CUT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
